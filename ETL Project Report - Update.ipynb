{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project: Landing the Google Interview\n",
    "\n",
    "   #### Exploring preferred qualifications for jobs at Google\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Overview \n",
    "\n",
    "Everyone knows that Google has a reputation for hard interviews. However, with high pay, and excellent perks, people are lining up at the door to try and get a job there. Because of supply and demand, Google has the ability to be extremeley picky. So, if you want to work at Google, what are the most important skills that you should work on in order to achieve your goal? Let's take a look. \n",
    "\n",
    "## Objective \n",
    "\n",
    "The goal of my project is to analyze the job postings and preferred qualifications to get a picture of what the ideal candidate looks like for a job at Google, ie. what qualifications are mentioned the most for certain job categories. So if you have your sights set on a job at Google, my analysis will show the most commonly seen skill sets to help guide the next steps of your skill aquisition process. (I should probably take my own advice here.) \n",
    "\n",
    "## Methodology \n",
    "\n",
    "I found a .CSV data set using Kaggle.com. Link: (https://www.kaggle.com/niyamatalmass/google-job-skills). The data set itself was a web-scrape of Google's job postings taken from '16 -'17. The user who posted it used Selenium to scrape the data. \n",
    "\n",
    "# Steps\n",
    "\n",
    "### Data Frame\n",
    "\n",
    "To perform the analysis, I loaded the .CSV into a Pandas data frame. The raw data looked like this: \n",
    "\n",
    "<img src=\"Google_Skills_Raw_Data.png\" width=\"800\" />\n",
    "\n",
    "### Data Cleaning & Transforming \n",
    "\n",
    "Next, I cleaned the data so that I could run my text analysis on the resulting dataframe. I used a function to strip out punctuation, EOL characters, & \"Stop Words\". I found a list of common stop words on Stackoverflow to create an easy list of which words to strip out. It otherwise would have been quite a manual process. \n",
    "\n",
    "After removing the words I don't want in the analysis, I integrated my new columns in my data frame. I got the bare-bone nouns and verbs I was looking for to run my text analysis here. This doesn't look like a big change, but I managed to strip out a lot of unnecessary words and transform each cell into a list of strings. \n",
    "\n",
    "<img src=\"Cleaned_Data_for_SQL.png\" width=\"800\" />\n",
    "\n",
    "### SQL Database\n",
    "\n",
    "After cleaning the data, it's time to upload to an SQL Database. Using the Postgres Admin client, I uploaded my new dataframe into an SQL Databse, which is shown here. \n",
    "\n",
    "<img src=\"PrefSkillsSQL_DB.png\" width=\"800\" />\n",
    "\n",
    "### Connecting to SQL Database & Building a New Dataframe for Analysis \n",
    "\n",
    "I wanted to run my analysis on strictly the preferred qualifications for \"Software Engineering\" at Google. So I set up my query from the SQL Databse to pull my data directly into Jupyter Notebook. Here's my database Query: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = engine.execute(\"Select preferred_qualifications from skilldetails WHERE category='Software Engineering'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After pulling my data, I built a new data frame. I looped through the data by Job Category, \"Software Engineering,\" and did unique word count of every individual word in my database, putting both the words and their corresponding unique counts into 2 lists. I used the 2 lists to create a new dataframe from which I would drive my analysis. \n",
    "\n",
    "<img src=\"PrefSkillsWordCount.png\" width=\"800\" />\n",
    "\n",
    "What I noticed here is that I still had a lot of redundant or otherwise irrelevant words in the database, such as, \"Experience,\" \"related,\" and \"nan.\" I chose to manually remove all rows that contained such words. I also noticed that because I sorted my dataframe in an ascending order, the index was out of order. I decided to reset the index as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {'Preferred_Skills': preferredSkills, 'Count': count}\n",
    "columns = ['Preferred_Skills', 'Count']\n",
    "df = pd.DataFrame(a, columns=columns)\n",
    "df2 = df.sort_values(by=\"Count\", ascending=False).reset_index(drop=True)\n",
    "#df2 = df.reset_index(drop=True)\n",
    "df3 = df2[df2.Preferred_Skills != 'nan']\n",
    "df4 = df3[df3.Preferred_Skills != 'Experience']\n",
    "df5 = df4[df4.Preferred_Skills != 'related']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resulted in the following dataframe: \n",
    "\n",
    "<img src=\"Reset_Index_Preferred_Skills.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis \n",
    "\n",
    "After building my new dataframe, here's a bar chart of the top \"40\" most common words from the Preferred Qualifications of all Software Engineering roles at Google. \n",
    "\n",
    "<img src=\"Top 40 Preferred Skills Clean.png\" width=\"800\" />\n",
    "\n",
    "Of all the job postings on Google's job board, here is the breakdown. At the time of this web scraping, there were only 31 Software Engineering Jobs available at Google. \n",
    "\n",
    "<img src=\"Google Jobs by Category.png\" width=\"800\" />\n",
    "\n",
    "It's important to take the number of jobs available into account and compare that to the graph above. Of the 31 jobs available for Software Engineering, the following Preferred Qualifications showed up more often in my analysis with their respective frequency. These are the qualifications that I hand selected from the list that jumped out at me as a qualification. \n",
    "\n",
    "Experience - 14 times (Duh) \n",
    "\n",
    "Technical - 13 times\n",
    "\n",
    "PhD - 11 times\n",
    "\n",
    "Computer Science - 11 times \n",
    "\n",
    "Design - 9 times \n",
    "\n",
    "Python - 7 times\n",
    "\n",
    "Development - 7 times\n",
    "\n",
    "Systems - 6 times\n",
    "\n",
    "Code - 6 times\n",
    "\n",
    "Coding - 4 times\n",
    "\n",
    "Languages - 5 times (implying multiple languages) \n",
    "\n",
    "C - 5 times\n",
    "\n",
    "JavaScript - 5 times\n",
    "\n",
    "Programming - 5 times\n",
    "\n",
    "GO - 4 times\n",
    "\n",
    "CC - 4 times\n",
    "\n",
    "MS - 4 times (Masters of Science)\n",
    "\n",
    "Masters - 4 times \n",
    "\n",
    "etc. \n",
    "\n",
    "### Conclusion\n",
    "\n",
    "If you want to work at Google as a Software Engineer, it might be time to back to school and get your PhD in Comp Sci. If you don't want to spend time doing that, maybe a Master's Degree in Computer Science will help add some weight to your resume. While you're at it, it would be a good idea to focus on learning multiple languages such as Python, JavaScript, Go, and C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
